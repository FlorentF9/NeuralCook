{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Cook\n",
    "\n",
    "_by [FlorentF9](http://github.com/FlorentF9)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from data_utils import Dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ID\n",
    "run = '140817'\n",
    "\n",
    "# Load data\n",
    "train = Dataset(file='data/recipes.txt')\n",
    "with open('dictionary_{}.pickle'.format(run), 'wb') as f:\n",
    "    pickle.dump(train.dictionary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_symbols = train.n_symbols\n",
    "n_steps = 10\n",
    "n_hidden = 128\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder('float', [None, n_steps, n_symbols])\n",
    "y = tf.placeholder('float', [None, n_symbols])\n",
    "\n",
    "# Define weights\n",
    "weights = tf.get_variable('weights', initializer=tf.random_normal([n_hidden, n_symbols]))\n",
    "biases = tf.get_variable('biases', initializer=tf.random_normal([n_symbols]))\n",
    "\n",
    "# Define operations\n",
    "prediction = RNN(x, weights, biases)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Add ops for Tensorboard\n",
    "tf.summary.scalar('loss', loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0.04640828852032973, Minibatch Loss= 2.809701, Training Accuracy= 0.28125\n",
      "Model saved in file: models/model_140817.ckpt\n",
      "Epoch 0.09281657704065946, Minibatch Loss= 2.283351, Training Accuracy= 0.40625\n",
      "Model saved in file: models/model_140817.ckpt\n",
      "Epoch 0.1392248655609892, Minibatch Loss= 2.080940, Training Accuracy= 0.43750\n",
      "Model saved in file: models/model_140817.ckpt\n",
      "Epoch 0.18563315408131892, Minibatch Loss= 2.785281, Training Accuracy= 0.28125\n",
      "Epoch 0.23204144260164866, Minibatch Loss= 1.979953, Training Accuracy= 0.62500\n",
      "Model saved in file: models/model_140817.ckpt\n",
      "Epoch 0.2784497311219784, Minibatch Loss= 2.132978, Training Accuracy= 0.40625\n",
      "Epoch 0.3248580196423081, Minibatch Loss= 1.838617, Training Accuracy= 0.56250\n",
      "Model saved in file: models/model_140817.ckpt\n",
      "Epoch 0.37126630816263784, Minibatch Loss= 1.635211, Training Accuracy= 0.59375\n",
      "Model saved in file: models/model_140817.ckpt\n",
      "Epoch 0.4176745966829676, Minibatch Loss= 1.650619, Training Accuracy= 0.50000\n",
      "Epoch 0.4640828852032973, Minibatch Loss= 2.078444, Training Accuracy= 0.53125\n",
      "Epoch 0.5104911737236271, Minibatch Loss= 1.978636, Training Accuracy= 0.43750\n",
      "Epoch 0.5568994622439568, Minibatch Loss= 1.852121, Training Accuracy= 0.50000\n",
      "Epoch 0.6033077507642866, Minibatch Loss= 1.895829, Training Accuracy= 0.43750\n",
      "Epoch 0.6497160392846162, Minibatch Loss= 1.794448, Training Accuracy= 0.43750\n",
      "Epoch 0.6961243278049459, Minibatch Loss= 1.325133, Training Accuracy= 0.65625\n",
      "Model saved in file: models/model_140817.ckpt\n",
      "Epoch 0.7425326163252757, Minibatch Loss= 1.420983, Training Accuracy= 0.68750\n",
      "Epoch 0.7889409048456054, Minibatch Loss= 1.723116, Training Accuracy= 0.56250\n",
      "Epoch 0.8353491933659352, Minibatch Loss= 1.479488, Training Accuracy= 0.53125\n",
      "Epoch 0.8817574818862649, Minibatch Loss= 1.934516, Training Accuracy= 0.34375\n",
      "Epoch 0.9281657704065946, Minibatch Loss= 1.567729, Training Accuracy= 0.53125\n",
      "Epoch 0.9745740589269244, Minibatch Loss= 2.127463, Training Accuracy= 0.50000\n",
      "Epoch 1.0209823474472541, Minibatch Loss= 1.755329, Training Accuracy= 0.53125\n",
      "Epoch 1.0673906359675838, Minibatch Loss= 1.574025, Training Accuracy= 0.53125\n",
      "Epoch 1.1137989244879136, Minibatch Loss= 1.450826, Training Accuracy= 0.65625\n",
      "Epoch 1.1602072130082433, Minibatch Loss= 1.343631, Training Accuracy= 0.62500\n",
      "Epoch 1.206615501528573, Minibatch Loss= 1.701327, Training Accuracy= 0.59375\n",
      "Epoch 1.2530237900489027, Minibatch Loss= 1.870075, Training Accuracy= 0.53125\n",
      "Epoch 1.2994320785692324, Minibatch Loss= 1.252992, Training Accuracy= 0.56250\n",
      "Model saved in file: models/model_140817.ckpt\n",
      "Epoch 1.3458403670895622, Minibatch Loss= 1.456030, Training Accuracy= 0.56250\n",
      "Epoch 1.3922486556098919, Minibatch Loss= 1.404662, Training Accuracy= 0.65625\n",
      "Epoch 1.4386569441302217, Minibatch Loss= 1.362361, Training Accuracy= 0.59375\n",
      "Epoch 1.4850652326505513, Minibatch Loss= 1.146623, Training Accuracy= 0.68750\n",
      "Model saved in file: models/model_140817.ckpt\n",
      "Epoch 1.5314735211708812, Minibatch Loss= 1.411948, Training Accuracy= 0.56250\n",
      "Epoch 1.5778818096912108, Minibatch Loss= 1.459531, Training Accuracy= 0.56250\n",
      "Epoch 1.6242900982115407, Minibatch Loss= 1.669577, Training Accuracy= 0.46875\n",
      "Epoch 1.6706983867318703, Minibatch Loss= 1.133262, Training Accuracy= 0.56250\n",
      "Model saved in file: models/model_140817.ckpt\n",
      "Epoch 1.7171066752522, Minibatch Loss= 1.029230, Training Accuracy= 0.75000\n",
      "Model saved in file: models/model_140817.ckpt\n",
      "Epoch 1.7635149637725298, Minibatch Loss= 1.133990, Training Accuracy= 0.68750\n",
      "Epoch 1.8099232522928594, Minibatch Loss= 1.312820, Training Accuracy= 0.62500\n",
      "Epoch 1.8563315408131893, Minibatch Loss= 1.333033, Training Accuracy= 0.62500\n",
      "Epoch 1.902739829333519, Minibatch Loss= 1.484269, Training Accuracy= 0.53125\n",
      "Epoch 1.9491481178538488, Minibatch Loss= 1.779420, Training Accuracy= 0.56250\n",
      "Epoch 1.9955564063741784, Minibatch Loss= 1.210226, Training Accuracy= 0.75000\n",
      "Epoch 2.0419646948945083, Minibatch Loss= 1.442393, Training Accuracy= 0.50000\n",
      "Epoch 2.0883729834148377, Minibatch Loss= 1.461855, Training Accuracy= 0.53125\n",
      "Epoch 2.1347812719351675, Minibatch Loss= 1.839873, Training Accuracy= 0.50000\n",
      "Epoch 2.1811895604554974, Minibatch Loss= 1.067797, Training Accuracy= 0.65625\n",
      "Epoch 2.2275978489758272, Minibatch Loss= 1.763355, Training Accuracy= 0.53125\n",
      "Epoch 2.2740061374961567, Minibatch Loss= 1.343095, Training Accuracy= 0.65625\n",
      "Epoch 2.3204144260164865, Minibatch Loss= 1.094101, Training Accuracy= 0.62500\n",
      "Epoch 2.3668227145368164, Minibatch Loss= 1.150669, Training Accuracy= 0.62500\n",
      "Epoch 2.413231003057146, Minibatch Loss= 1.189107, Training Accuracy= 0.62500\n",
      "Epoch 2.4596392915774756, Minibatch Loss= 2.000163, Training Accuracy= 0.59375\n",
      "Epoch 2.5060475800978055, Minibatch Loss= 0.810536, Training Accuracy= 0.75000\n",
      "Model saved in file: models/model_140817.ckpt\n",
      "Epoch 2.5524558686181353, Minibatch Loss= 1.290352, Training Accuracy= 0.62500\n",
      "Epoch 2.5988641571384647, Minibatch Loss= 0.893180, Training Accuracy= 0.90625\n",
      "Epoch 2.6452724456587946, Minibatch Loss= 0.867087, Training Accuracy= 0.71875\n",
      "Epoch 2.6916807341791245, Minibatch Loss= 1.283438, Training Accuracy= 0.65625\n",
      "Epoch 2.7380890226994543, Minibatch Loss= 1.054669, Training Accuracy= 0.68750\n",
      "Epoch 2.7844973112197837, Minibatch Loss= 1.494000, Training Accuracy= 0.59375\n",
      "Epoch 2.8309055997401136, Minibatch Loss= 1.012295, Training Accuracy= 0.71875\n",
      "Epoch 2.8773138882604434, Minibatch Loss= 1.098660, Training Accuracy= 0.71875\n",
      "Epoch 2.923722176780773, Minibatch Loss= 1.010886, Training Accuracy= 0.78125\n",
      "Epoch 2.9701304653011027, Minibatch Loss= 1.247221, Training Accuracy= 0.71875\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 3\n",
    "EVAL_FREQUENCY = 1000\n",
    "batch_size = 32\n",
    "train_size = len(train.data)\n",
    "best_loss = np.inf\n",
    "\n",
    "batch_gen = train.batch(batch_size, n_steps)\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('logs/'+run, sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size / train_size < N_EPOCHS:\n",
    "        batch_x, batch_y = next(batch_gen)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % EVAL_FREQUENCY == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            summary, loss_, acc_ = sess.run([merged, loss, accuracy], feed_dict={x: batch_x, y: batch_y})\n",
    "            writer.add_summary(summary, step)\n",
    "            print(\"Epoch \" + str(step * batch_size / train_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc_))\n",
    "            # Save weights\n",
    "            if loss_ < best_loss:\n",
    "                best_loss = loss_\n",
    "                save_path = saver.save(sess, \"models/model_{}.ckpt\".format(run))\n",
    "                print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 10 characters: Etape 2: F\n",
      "Etape 2: Faire vercou en de le ter couper les poit encres de poitre fermit de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de vertir de ve\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"models/model_{}.ckpt\".format(run))\n",
    "    prompt = \"Enter {} characters: \".format(n_steps)\n",
    "    characters = input(prompt)\n",
    "    if len(characters) != n_steps:\n",
    "        print('Please enter {} characters.'.format(n_steps))\n",
    "    try:\n",
    "        symbols_in_keys = [train.dictionary[characters[i]] for i in range(len(characters))]\n",
    "        for i in range(300):\n",
    "            symbols_onehot = np.zeros((1, n_steps, n_symbols))\n",
    "            for i in range(n_steps):\n",
    "                symbols_onehot[0, i, symbols_in_keys[i]] = 1.0\n",
    "            onehot_pred = sess.run(prediction, feed_dict={x: symbols_onehot})\n",
    "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "            characters += train.reverse_dictionary[onehot_pred_index]\n",
    "            symbols_in_keys = symbols_in_keys[1:]\n",
    "            symbols_in_keys.append(onehot_pred_index)\n",
    "        print(characters)\n",
    "    except:\n",
    "        print('Not in dictionary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
